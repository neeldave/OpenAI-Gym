{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neeldave/conda/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode 1 completed with total reward 7871.170184629269 in 25282 steps\n",
      "Episode 2 completed with total reward 7905.6806224591255 in 24729 steps\n",
      "Episode 3 completed with total reward 1579.09754758362 in 4660 steps\n",
      "Episode 4 completed with total reward 597.3158843017146 in 2035 steps\n",
      "Episode 5 completed with total reward 354.33633750944034 in 1574 steps\n",
      "Episode 6 completed with total reward 2360.0859538215186 in 6564 steps\n",
      "Episode 7 completed with total reward 1755.9848830562498 in 4626 steps\n",
      "Episode 8 completed with total reward 1913.8973035918755 in 7693 steps\n",
      "Episode 9 completed with total reward 304.4790819474709 in 1055 steps\n",
      "Episode 10 completed with total reward 570.2386276991492 in 2014 steps\n"
     ]
    }
   ],
   "source": [
    "# importing the dependencies\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# exploring Mountain Car environment\n",
    "\n",
    "envName = 'MountainCar-v0'\n",
    "env = gym.make(envName)\n",
    "\n",
    "n_states = 40  # number of states\n",
    "episodes = 10 # number of episodes\n",
    "\n",
    "intialLr = 1.0 # initial learning rate\n",
    "minimumLr = 0.005 # minimum learning rate\n",
    "gamma = 0.99 # discount factor\n",
    "maxSteps = 300\n",
    "epsilon = 0.05\n",
    "\n",
    "env = env.unwrapped\n",
    "env.seed(0)         # setting environment seed to reproduce same result\n",
    "np.random.seed(0)   #setting numpy random number generation seed to reproduce same random numbers\n",
    "\n",
    "def Discretization(env, obs):\n",
    "    \n",
    "    envLow = env.observation_space.low\n",
    "    envHigh = env.observation_space.high\n",
    "    \n",
    "    envDen = (envHigh - envLow) / n_states\n",
    "    posDen = envDen[0]\n",
    "    velDen = envDen[1]\n",
    "    \n",
    "    posHigh = envHigh[0]\n",
    "    posLow = envLow[0]\n",
    "    velHigh = envHigh[1]\n",
    "    velLow = envLow[1]\n",
    "    \n",
    "    pos_scaled = int((obs[0] - posLow)/posDen)  #converts to an integer value\n",
    "    vel_scaled = int((obs[1] - velLow)/velDen)  #converts to an integer value\n",
    "    \n",
    "    return pos_scaled,vel_scaled\n",
    "\n",
    "q_table = np.zeros((n_states,n_states,env.action_space.n))\n",
    "total_steps = 0\n",
    "for episode in range(episodes):\n",
    "      obs = env.reset()\n",
    "      total_reward = 0\n",
    "      # decreasing learning rate alpha over time\n",
    "      alpha = max(minimumLr,intialLr*(gamma**(episode//100)))\n",
    "      steps = 0\n",
    "      while True:\n",
    "          env.render()\n",
    "          pos,vel = Discretization(env,obs)\n",
    "          \n",
    "          #action for the current state using epsilon greedy\n",
    "          if np.random.uniform(low=0,high=1) < epsilon:\n",
    "                a = np.random.choice(env.action_space.n)\n",
    "          else:\n",
    "                a = np.argmax(q_table[pos][vel])\n",
    "          obs,reward,terminate,_ = env.step(a) \n",
    "          total_reward += abs(obs[0]+0.5)\n",
    "    \n",
    "          #q-table update\n",
    "          pos_,vel_ = Discretization(env,obs)\n",
    "          q_table[pos][vel][a] = (1-alpha)*q_table[pos][vel][a] + alpha*(reward+gamma*np.max(q_table[pos_][vel_]))\n",
    "          steps+=1\n",
    "          if terminate:\n",
    "                break\n",
    "      print(\"Episode {} completed with total reward {} in {} steps\".format(episode+1,total_reward,steps)) \n",
    "\n",
    "while True: #to hold the render at the last step when Car passes the flag\n",
    "      env.render() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
